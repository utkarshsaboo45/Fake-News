{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e6deffe-9c9a-4a8b-8fb8-f2c2608fc4eb",
   "metadata": {},
   "source": [
    "# Fake News Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8729962c-4ce7-47d5-9c43-44f06b367537",
   "metadata": {},
   "source": [
    "### Data Reading and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4573692a-e238-4943-a9bc-d270a041a2f1",
   "metadata": {},
   "source": [
    "__Import Statements__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "efc873ea-5618-4d36-804d-88a062312ede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataTransformerRegistry.enable('data_server')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "from os import path\n",
    "import re\n",
    "import string\n",
    "import altair as alt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "\n",
    "# nltk.download('words')\n",
    "# nltk.download(\"cmudict\")\n",
    "# nltk.download(\"vader_lexicon\")\n",
    "# nltk.download(\"punkt\")\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import cmudict, stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#from similarity.jarowinkler import JaroWinkler\n",
    "#from similarity.cosine import Cosine\n",
    "#from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import (\n",
    "    OneHotEncoder,\n",
    "    StandardScaler,\n",
    "    FunctionTransformer\n",
    ")\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "alt.data_transformers.enable('data_server')\n",
    "# alt.renderers.enable('mimetype')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e95dd17-6ad4-42fc-9745-a424d5690282",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/raw/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724ffc8a-2011-4472-8056-23be44fbb12f",
   "metadata": {},
   "source": [
    "### Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb239a-5edc-4d2a-82fe-f65b79be2fd2",
   "metadata": {},
   "source": [
    "- Deal NaNs/Nulls and empty texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b3ea891-060a-4e64-a7bb-4dff4fca7055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(word):\n",
    "    punctuations = string.punctuation\n",
    "    punctuations += \"“”–\\n\"\n",
    "\n",
    "    for element in word:\n",
    "        if element in punctuations:\n",
    "            word = word.replace(element, \"\")\n",
    "    return word\n",
    "\n",
    "\n",
    "def clean_data(text):\n",
    "    text = str(text).lower()\n",
    "    text = str(text).strip()\n",
    "    text\n",
    "    text = re.sub(\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "    text = remove_punctuation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15843fd4-dd50-4f42-8973-5295cea0b6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3efc74f5-2210-4e8f-af6e-8aff86610904",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"title\"] = train_df[\"title\"].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5807218-3bc0-42f7-9b60-e1fa4157a445",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"text\"] = train_df[\"text\"].apply(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b1674-2854-4308-a9a5-f482a86c4525",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0b970-b262-4c08-8628-e556fc09cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbfcdd-d0fd-461e-8d2e-5f6b8af47e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060555c8-4c1e-4653-ab65-21ebdb7a5d6c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7625a8d8-da4f-472d-937b-f692a6cc6f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"label\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102b5cd-6f44-4507-b535-a20ecf1bdfa3",
   "metadata": {},
   "source": [
    "__There is no class imbalance__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1586a6a8-da5d-43c6-9f40-8b9809807f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fake = \" \".join(train_df[train_df[\"label\"] == 1][\"text\"])\n",
    "real = \" \".join(train_df[train_df[\"label\"] == 0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c111cb1a-2fa2-4fc9-b607-7e5d3dd19590",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"English\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676e7b53-9244-4845-a41c-30670f555d00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wordcloud_fake = WordCloud(\n",
    "#     stopwords=stop_words,\n",
    "#     max_font_size=40,\n",
    "#     width=400,\n",
    "#     height=200\n",
    "# ).generate(fake).to_image()\n",
    "\n",
    "# plt.imshow(wordcloud_fake)\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e932b756-4f6b-49d1-8409-04602c9feb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud_real = WordCloud(\n",
    "#     stopwords=stop_words,\n",
    "#     max_font_size=40,\n",
    "#     width=400,\n",
    "#     height=200\n",
    "# ).generate(real).to_image()\n",
    "\n",
    "# plt.imshow(wordcloud_real)\n",
    "# plt.axis(\"off\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcd3a61-9f3b-4122-93f9-be5db63c3e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_counter = Counter(word for word in real.split(\" \") if word not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb880641-c44b-4376-abb1-6a9f27443b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_counter = Counter(word for word in fake.split(\" \") if word not in stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4cb84d4-a5e7-4311-8796-b1bc41fc54c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fake[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d7d72-2aa8-4bc1-9f3d-d2e34945440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_real = pd.DataFrame.from_dict(\n",
    "#     real_counter,\n",
    "#     orient=\"index\",\n",
    "#     columns=[\"count\"]\n",
    "# ).reset_index().sort_values(\"count\", ascending=False).head(20)\n",
    "\n",
    "# df_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80374e91-7e97-4bff-825e-548ec5c9e8f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_fake = pd.DataFrame.from_dict(\n",
    "#     fake_counter,\n",
    "#     orient=\"index\",\n",
    "#     columns=[\"count\"]\n",
    "# ).reset_index().sort_values(\"count\", ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65468305-3811-4ce8-8448-a7f2bd54c88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.Chart(df_real, title=\"Frequency of top 10 words in Real News\").mark_bar().encode(\n",
    "#     x=\"index\",\n",
    "#     y=\"counts\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40af059-15cd-4ff4-81cf-c4fa540f0449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.Chart(df_fake, title=\"Frequency of top 10 words in Fake News\").mark_bar().encode(\n",
    "#     alt.X(\"index\", sort=\"-y\"),\n",
    "#     alt.Y(\"count\")\n",
    "# )\n",
    "\n",
    "# plot = sns.countplot(x=\"index\", data = df_fake)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341e4920-a375-4fc0-937f-571e9e52fa23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62673d40-7482-4569-93c4-37e77a17d212",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Preprocessing and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21864ca-d191-4c20-b8f9-30500a35be98",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Author Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f575df-4fcb-4539-8833-732ac1f8732e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature \"author_is_null\"\n",
    "\n",
    "# def author_is_null(x):\n",
    "#     if x[\"author\"] != x[\"author\"]:\n",
    "#         return 0\n",
    "#     return 1\n",
    "\n",
    "# train_df[\"author_is_null\"] = train_df.apply(lambda x: author_is_null(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73ddc58-1d4a-4716-a8e3-05ccee5670d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4adb788-5161-4ef8-9dc4-96fe72c0fb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Author = Null to Author = \"Unknown\"\n",
    "\n",
    "# unknown_authors_ids = train_df.query(\"author.isnull()\")[\"id\"]\n",
    "# train_df['updated_author_column_name'] = np.where(~train_df['id'].isin(unknown_authors_ids), train_df['author'], 'Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd0fe34-705e-4b25-ae4b-1cbbf1d9edf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fe8d1a-9152-4587-8d89-cf5fc47947a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Others category if value_counts of an author is less than 5\n",
    "\n",
    "# less_frequent = train_df['author'].value_counts()[train_df['author'].value_counts() <= 5].index.tolist()\n",
    "# train_df['author'] = np.where(train_df['author'].isin(less_frequent), 'Other', train_df['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102e3fc-7242-4fd8-9ad9-bd4e8f0debfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c140a1dc-f0c3-4b85-bd98-83d9c22fea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create if \"is_multiple_authors\"\n",
    "\n",
    "# def is_multiple_authors(data):\n",
    "\n",
    "#     data[\"is_multiple_authors\"] = [\n",
    "#         1 if \" and \" in str(author) else 0 for author in data[\"author\"]\n",
    "#     ]\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# train_df = is_multiple_authors(train_df)\n",
    "\n",
    "# train_df.query(\"is_multiple_authors == 1\")[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c1ba8-e549-4e56-85b8-10e30814fe71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd2ba6-3e63-4f63-b1e1-cc52a3f996c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if author name contains a domain suffix\n",
    "\n",
    "# def author_contains_domain(data):\n",
    "\n",
    "#     data[\"author_contains_domain\"] = [\n",
    "#         1 if re.search(r\"\\.[a-zA-Z]{3}\", str(author)) else 0 for author in train_df[\"author\"]\n",
    "#     ]\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# train_df = author_contains_domain(train_df)\n",
    "\n",
    "# train_df.query(\"author_contains_domain == 1\")[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5731aee0-121c-44ec-b2b1-aedf8e93351b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91d952f0-399b-4f62-b625-0fd472b077b9",
   "metadata": {},
   "source": [
    "#### Title Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63ecf1-b40f-48e8-a97a-d1ed391c143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if title is null\n",
    "\n",
    "# def is_title_null(data):\n",
    "\n",
    "#     data[\"is_title_null\"] = [\n",
    "#         0 if title == title\n",
    "#         else 1 for title in train_df[\"title\"]\n",
    "#     ]\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# train_df = is_title_null(train_df)\n",
    "\n",
    "# train_df.query(\"is_title_null == 1\")[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2c143d-dda3-4f3b-8fb4-4b5cd625d219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85295f28-54d8-4ec9-852c-8d27fcbccfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if title ends with a famous journal name\n",
    "\n",
    "# def title_contains_famous_journal(data):\n",
    "\n",
    "#     data[\"title_contains_famous_journal\"] = [\n",
    "#         1 if\n",
    "#         str(title).endswith(\"The New York Times\") or\n",
    "#         str(title).endswith(\"Breitbart\")\n",
    "#         else 0 for title in train_df[\"title\"]\n",
    "#     ]\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# train_df = title_contains_famous_journal(train_df)\n",
    "\n",
    "# train_df.query(\"title_contains_famous_journal == 1\")[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30be091-622d-4155-9e8a-dd606727d2a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa58a3-237e-4dda-a643-1e4181c525d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def no_of_words(data):\n",
    "\n",
    "#     data[\"no_of_words\"] = [\n",
    "#         len(str(title).split(\" \")) for title in train_df[\"title\"]\n",
    "#     ]\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# train_df = no_of_words(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffae63b-4e57-4613-9706-ba19836f64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.Chart(train_df).mark_bar().encode(\n",
    "#     alt.X(\"no_of_words\", bin=alt.Bin(maxbins=50)),\n",
    "#     alt.Y(\"count()\"),\n",
    "#     color=\"label\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e4e982-53a7-4a89-9946-8db5937c4f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14eef46-2d46-4871-8eaa-59fa789de226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def no_of_chars(data):\n",
    "\n",
    "#     data[\"no_of_chars\"] = [\n",
    "#         len(str(title)) for title in train_df[\"title\"]\n",
    "#     ]\n",
    "\n",
    "#     return data\n",
    "\n",
    "\n",
    "# train_df = no_of_chars(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba79f5-88c8-420d-a9c3-0dd4c38cbba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7a7c36-8dd0-4991-af71-99ed33a1a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.Chart(train_df).mark_bar().encode(\n",
    "#     alt.X(\"no_of_chars\", bin=alt.Bin(maxbins=100)),\n",
    "#     alt.Y(\"count()\"),\n",
    "#     color=\"label\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd06d160-0bce-4e32-8303-f220b6c5b733",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b55bd27-276b-4d3f-a590-e3f37b4052bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_text_length(text):\n",
    "#     \"\"\"\n",
    "#     Returns the number of words in a text without punctuations. \n",
    "#     Counts clitics as separate words.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     text : str\n",
    "#         A text from which we find the number of words\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     An int which represents the number of words in the text\n",
    "#     \"\"\"\n",
    "#     non_punc = []\n",
    "#     for word in word_tokenize(str(text)):\n",
    "#         if word not in string.punctuation:\n",
    "#             non_punc.append(word)\n",
    "#     return len(non_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280fbbe5-0134-4cf8-bfaa-f058a2ecf9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df.assign(title_len=train_df[\"title\"].apply(get_text_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312c4aa-ffb2-486d-9d3f-43f94e8fe9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad8b6a-7961-4f68-a70b-54db1f215d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_lexical_density(text):\n",
    "#     \"\"\"\n",
    "#     Returns the lexical density of a text. That is the ratio of open class words.\n",
    "#     in the text\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     text : str\n",
    "#         A text from which we find the lexical density\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     A float which represents the lexical density\n",
    "#     \"\"\"\n",
    "#     open_class_prefix = {\"N\", \"V\", \"J\", \"R\"}\n",
    "#     open_class_total = 0\n",
    "#     word_count = 0\n",
    "#     if len(str(text)) == 0:\n",
    "#         return float(0)\n",
    "#     for word, pos in pos_tag(word_tokenize(str(text))):\n",
    "#         if word not in string.punctuation:\n",
    "#             word_count += 1\n",
    "#             if pos[0] in open_class_prefix:\n",
    "#                 open_class_total += 1\n",
    "#     return open_class_total/word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd42c14-8bef-467b-8250-4529282ab5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"title_lexical_density\"] = train_df[\"title\"].apply(get_lexical_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fee541-6649-49c2-9e9e-be497f94f94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[\"text_lexical_density\"] = train_df[\"text\"].apply(get_lexical_density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca00ffc4-b816-488b-adee-ab218d351c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_pos_count(text):\n",
    "#     \"\"\"\n",
    "#     Counts the number of nouns, verbs and adjectives in a text.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     text : str\n",
    "#         A text for which we find the number of nouns, verbs\n",
    "#         and adjectives\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     A tuple of (noun_count: int, verb_count: int, adj_count: int)\n",
    "#     which represents the number of nouns, verbs adjectives in the text\n",
    "#     respectively\n",
    "#     \"\"\"\n",
    "#     noun_count = 0\n",
    "#     verb_count = 0\n",
    "#     adj_count = 0\n",
    "\n",
    "#     if len(str(text)) == 0:\n",
    "#         return 0, 0, 0\n",
    "\n",
    "#     for word, pos in pos_tag(word_tokenize(str(text))):\n",
    "#         if(pos[0] == 'N'):\n",
    "#             noun_count += 1\n",
    "#         if(pos[0] == 'V'):\n",
    "#             verb_count += 1\n",
    "#         if(pos == 'JJ'):\n",
    "#             adj_count += 1\n",
    "#     return noun_count\n",
    "#     return verb_count\n",
    "#     return adj_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0696f3-ef17-4d0f-8c65-648c6d91b70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"count_pos_title\"] = train_df[\"title\"].apply(get_pos_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190ce4b9-c202-44d2-9c3d-63c4ea64e090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"count_noun_title\"], train_df[\"count_verb_title\"], train_df[\"count_adj_title\"] = train_df[\"count_pos_title\"].str[0],train_df[\"count_pos_title\"].str[1],train_df[\"count_pos_title\"].str[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d4900d-4e20-4957-9588-478d7c998405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_num_ovv_words(text):\n",
    "#     \"\"\"\n",
    "#     Gets the number of out-of-vocabulary words in a text.\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     text : str\n",
    "#         A text for which we find the number of out-of-vocabulary\n",
    "#         words is to be found\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     The number of oov words in the text\n",
    "#     \"\"\"\n",
    "#     text_vocab = set(w for w in text.split() if w.isalpha())\n",
    "#     english_vocab = set(w for w in nltk.corpus.words.words())\n",
    "#     ovv_words = text_vocab - english_vocab\n",
    "\n",
    "#     return len(ovv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fe8bfa-5683-4e37-8660-9961ca72e6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df[\"title_ovw\"] = train_df[\"title\"].apply(get_num_ovv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8710a2-6745-4b9e-9e7a-8581ef7f1bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def contains_says(data):\n",
    "\n",
    "#     data[\"contains_says\"] = [\n",
    "#         1 if\n",
    "#         len(str(title).split(\" \")) < 6\n",
    "\n",
    "# #         re.search(\"[^a-zA-Z0-9 .,:'\\\"-\\\\$()]\", str(title))\n",
    "\n",
    "# #         re.search(\"[0-9]\", str(title))\n",
    "\n",
    "# #         \"Says\" in str(title) or \"says\" in str(title)\n",
    "\n",
    "#         else 0 for title in train_df[\"title\"]\n",
    "#     ]\n",
    "\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d24fae8-2595-4d7b-ad47-14c1b11d7c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = contains_says(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea522e-2019-43a9-aedc-9620a9d10da3",
   "metadata": {},
   "source": [
    "#### Text Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4416f162-6ae2-4f3b-9b6b-0ed76d2b4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check if text is empty\n",
    "\n",
    "# def is_text_empty(data):\n",
    "\n",
    "#     data[\"is_text_empty\"] = [\n",
    "#         1 if text == \" \" or\n",
    "#         not text == text\n",
    "#         else 0 for text in train_df[\"text\"]\n",
    "#     ]\n",
    "\n",
    "#     return data\n",
    "# train_df.query(\"text == ' '\")[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca65363a-61b9-42f0-ac16-2b2693450e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df = is_text_empty(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590515b7-8e51-4dde-9dba-b44a36306abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df2.query(\"is_text_empty == 1\")[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c929a019-24c8-4cfe-8361-91959314846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[\"text_len\"] = train_df[\"text\"].apply(get_text_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853a27fc-0135-456e-bbc3-7c585b49d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[\"text_ovw\"] = train_df[\"text\"].apply(get_num_ovv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086216ea-8048-45f7-9493-fffb99a7b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df[\"text_pos_count\"] = train_df[\"text\"].apply(get_pos_count)\n",
    "#train_df[\"count_noun_text\"], train_df[\"count_verb_text\"], train_df[\"count_adj_text\"] = train_df[\"text_pos_count\"].str[0],train_df[\"text_pos_count\"].str[1],train_df[\"text_pos_count\"].str[2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9300b763-17d3-40e1-b236-a96c198ec6c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533eeaff-c9e2-4916-bb96-05c0d6e0bd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11de4f4-9071-4245-aac9-b7e84a2e0b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(\"temp.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1498ded5-647c-4206-a0cd-1fbb89d3e3f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cosine Similarity between Title and Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e1a9b-e20c-4534-b025-4b5a6734ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine = Cosine(2)\n",
    "#train_df[\"p0\"] = train_df[\"title\"].apply(lambda s: cosine.get_profile(s))\n",
    "#train_df[\"p1\"] = train_df[\"text\"].apply(lambda s: cosine.get_profile(s))\n",
    "# train_df[\"cosine_sim\"] = [cosine.similarity_profiles(p0,p1) for p0, p1 in zip(train_df[\"p0\"],train_df[\"p1\"])]\n",
    "\n",
    "# train_df.drop([\"p0\", \"p1\"], axis=1)\n",
    "\n",
    "# jarowinkler = JaroWinkler()\n",
    "# df[\"jarowinkler_sim\"] = [jarowinkler.similarity(i,j) for i,j in zip(train_df[\"title\"],train_df[\"text\"])]\n",
    "\n",
    "#score = cosine_similarity(train_df['title'], train_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b07587-233d-48c9-8db2-23fb51e55639",
   "metadata": {},
   "source": [
    "### Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bdcaa3d-cf72-401a-b9d4-061f07beaa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'author', 'text', 'label'], dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1816a3c0-f36c-4717-a33e-62c4e2a07c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_small, val_df = train_test_split(train_df, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a5fbcc-5780-4cb6-9ce6-c3f1c07df16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df_small.drop(columns=[\"label\"]), train_df_small[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e546e26-5a79-4759-8f8a-99488a5439d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[\"text\"] = X_train[\"text\"].values.astype(\"U\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "47eadabf-601b-41a5-bab9-6e8f2fc8e483",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features1 = [\"title\"] \n",
    "text_features2 = [\"text\"]\n",
    "pass_through = []\n",
    "drop = [\"id\", \"author\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7ef11645-82e2-44d1-bacd-de9e8e6ef349",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_transformer = FunctionTransformer(\n",
    "    np.reshape, kw_args={\"newshape\": -1}\n",
    ")\n",
    "enc1 = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    function_transformer,\n",
    "    CountVectorizer(stop_words=\"english\", max_features=1000)\n",
    ")\n",
    "\n",
    "enc2 = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    function_transformer,\n",
    "    CountVectorizer(stop_words=\"english\", max_features=1000)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3155ba4c-8c26-4430-90de-166728124cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipe = make_pipeline(enc)\n",
    "preprocessor = make_column_transformer(\n",
    "    (enc1, text_features1),\n",
    "    (enc2, text_features2),\n",
    "    (\"drop\", drop)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4372d9f-1707-4085-87e2-35ea767387b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ef2799c0-ce86-46d5-a259-38d9c6bc6864",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                 ('functiontransformer',\n",
       "                                                  FunctionTransformer(func=<function reshape at 0x10fd91750>,\n",
       "                                                                      kw_args={'newshape': -1})),\n",
       "                                                 ('countvectorizer',\n",
       "                                                  CountVectorizer(max_features=1000,\n",
       "                                                                  stop_words='english'))]),\n",
       "                                 ['title']),\n",
       "                                ('pipeline-2',\n",
       "                                 Pipeline(steps=[('simpleimputer',\n",
       "                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                 ('functiontransformer',\n",
       "                                                  FunctionTransformer(func=<function reshape at 0x10fd91750>,\n",
       "                                                                      kw_args={'newshape': -1})),\n",
       "                                                 ('countvectorizer',\n",
       "                                                  CountVectorizer(max_features=1000,\n",
       "                                                                  stop_words='english'))]),\n",
       "                                 ['text']),\n",
       "                                ('drop', 'drop', ['id', 'author'])])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "bee18410-2a08-4ef4-a9b3-b85a987ea7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = (\n",
    "    preprocessor.named_transformers_[\"pipeline-1\"].named_steps[\"countvectorizer\"].get_feature_names_out().tolist() + \n",
    "    preprocessor.named_transformers_[\"pipeline-2\"].named_steps[\"countvectorizer\"].get_feature_names_out().tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "5763876e-e584-4865-b898-6e74f480fa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transfomed = preprocessor.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e3a02c45-5c70-4965-ae03-12a30fef99d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>15</th>\n",
       "      <th>20</th>\n",
       "      <th>2016</th>\n",
       "      <th>2017</th>\n",
       "      <th>30</th>\n",
       "      <th>50</th>\n",
       "      <th>...</th>\n",
       "      <th>worth</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>на</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16635</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16636</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16637</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16638</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16639</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16640 rows × 2000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       10  100  11  12  15  20  2016  2017  30  50  ...  worth  written  \\\n",
       "0       0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "1       0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "2       0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "3       0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "4       0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "...    ..  ...  ..  ..  ..  ..   ...   ...  ..  ..  ...    ...      ...   \n",
       "16635   0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "16636   0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "16637   0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "16638   0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "16639   0    0   0   0   0   0     0     0   0   0  ...      0        0   \n",
       "\n",
       "       wrong  wrote  year  years  yes  york  young  на  \n",
       "0          0      0     0      0    0     0      0   0  \n",
       "1          0      0     0      0    0     0      0   0  \n",
       "2          0      0     0      3    0     4      1   0  \n",
       "3          0      0     0      0    0     0      0   0  \n",
       "4          0      0     0      1    0     0      0   0  \n",
       "...      ...    ...   ...    ...  ...   ...    ...  ..  \n",
       "16635      0      0     1      0    0     1      0   0  \n",
       "16636      0      0     1      0    0     0      1   0  \n",
       "16637      1      0     0      1    0     0      0   0  \n",
       "16638      0      0     1      1    1     0      0   0  \n",
       "16639      0      0     0      0    0     0      0   0  \n",
       "\n",
       "[16640 rows x 2000 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(X_transfomed.toarray(), columns=new_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d0bb81-ac6d-4ec7-9728-0de219a97db0",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d20045-884b-429a-8a92-8f032c9923b4",
   "metadata": {},
   "source": [
    "__Split into Training and Validation data__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe43d5-b2a4-45bd-bd34-429bbdb12af2",
   "metadata": {},
   "source": [
    "#### Base models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "49c042eb-d700-4b2b-bf52-10374e952650",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(preprocessor, LogisticRegression(max_iter=100000))\n",
    "#pipe_dt = make_pipeline(preprocessor, DecisionTreeClassifier())\n",
    "pipe_nb = make_pipeline(\n",
    "    preprocessor,\n",
    "    FunctionTransformer(lambda x: x.todense(), accept_sparse=True),\n",
    "    GaussianNB())\n",
    "#pipe_svc = make_pipeline(preprocessor, SVC())\n",
    "pipe_rf = make_pipeline(preprocessor, RandomForestClassifier())\n",
    "pipe_catboost = make_pipeline(preprocessor, CatBoostClassifier(verbose=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2a022c04-a208-4f51-8740-727183df048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": pipe_lr,\n",
    "    #\"Decision Tree\": pipe_dt,\n",
    "    \"NB\": pipe_nb,\n",
    "    #\"SVC\": pipe_svc,\n",
    "    \"Random Forest\": pipe_rf,\n",
    "    \"Cat boost\": pipe_catboost\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8148cd3b-cee7-4286-ad18-b3918dbac42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f63a9d6-17c7-4664-90a4-00c036bf7ff1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ce2f7fda-3dba-41c2-beb3-f41c24d981d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Logistic Regression!\n",
      "Done Logistic Regression!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>7.271 (+/- 0.306)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.613 (+/- 0.024)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_score</th>\n",
       "      <td>0.964 (+/- 0.007)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_score</th>\n",
       "      <td>0.998 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Logistic Regression\n",
       "fit_time      7.271 (+/- 0.306)\n",
       "score_time    0.613 (+/- 0.024)\n",
       "test_score    0.964 (+/- 0.007)\n",
       "train_score   0.998 (+/- 0.000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start NB!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n",
      "/Users/valliakella/opt/miniconda3/envs/fake_news/lib/python3.10/site-packages/sklearn/utils/validation.py:593: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done NB!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>NB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>7.271 (+/- 0.306)</td>\n",
       "      <td>6.866 (+/- 0.146)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.613 (+/- 0.024)</td>\n",
       "      <td>0.656 (+/- 0.025)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_score</th>\n",
       "      <td>0.964 (+/- 0.007)</td>\n",
       "      <td>0.869 (+/- 0.008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_score</th>\n",
       "      <td>0.998 (+/- 0.000)</td>\n",
       "      <td>0.889 (+/- 0.003)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Logistic Regression                 NB\n",
       "fit_time      7.271 (+/- 0.306)  6.866 (+/- 0.146)\n",
       "score_time    0.613 (+/- 0.024)  0.656 (+/- 0.025)\n",
       "test_score    0.964 (+/- 0.007)  0.869 (+/- 0.008)\n",
       "train_score   0.998 (+/- 0.000)  0.889 (+/- 0.003)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Random Forest!\n",
      "Done Random Forest!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>NB</th>\n",
       "      <th>Random Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>7.271 (+/- 0.306)</td>\n",
       "      <td>6.866 (+/- 0.146)</td>\n",
       "      <td>17.725 (+/- 0.089)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.613 (+/- 0.024)</td>\n",
       "      <td>0.656 (+/- 0.025)</td>\n",
       "      <td>0.653 (+/- 0.018)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_score</th>\n",
       "      <td>0.964 (+/- 0.007)</td>\n",
       "      <td>0.869 (+/- 0.008)</td>\n",
       "      <td>0.973 (+/- 0.005)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_score</th>\n",
       "      <td>0.998 (+/- 0.000)</td>\n",
       "      <td>0.889 (+/- 0.003)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Logistic Regression                 NB       Random Forest\n",
       "fit_time      7.271 (+/- 0.306)  6.866 (+/- 0.146)  17.725 (+/- 0.089)\n",
       "score_time    0.613 (+/- 0.024)  0.656 (+/- 0.025)   0.653 (+/- 0.018)\n",
       "test_score    0.964 (+/- 0.007)  0.869 (+/- 0.008)   0.973 (+/- 0.005)\n",
       "train_score   0.998 (+/- 0.000)  0.889 (+/- 0.003)   1.000 (+/- 0.000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Cat boost!\n",
      "Done Cat boost!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistic Regression</th>\n",
       "      <th>NB</th>\n",
       "      <th>Random Forest</th>\n",
       "      <th>Cat boost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>fit_time</th>\n",
       "      <td>7.271 (+/- 0.306)</td>\n",
       "      <td>6.866 (+/- 0.146)</td>\n",
       "      <td>17.725 (+/- 0.089)</td>\n",
       "      <td>27.518 (+/- 0.807)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>score_time</th>\n",
       "      <td>0.613 (+/- 0.024)</td>\n",
       "      <td>0.656 (+/- 0.025)</td>\n",
       "      <td>0.653 (+/- 0.018)</td>\n",
       "      <td>0.632 (+/- 0.033)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_score</th>\n",
       "      <td>0.964 (+/- 0.007)</td>\n",
       "      <td>0.869 (+/- 0.008)</td>\n",
       "      <td>0.973 (+/- 0.005)</td>\n",
       "      <td>0.976 (+/- 0.004)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_score</th>\n",
       "      <td>0.998 (+/- 0.000)</td>\n",
       "      <td>0.889 (+/- 0.003)</td>\n",
       "      <td>1.000 (+/- 0.000)</td>\n",
       "      <td>0.990 (+/- 0.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Logistic Regression                 NB       Random Forest  \\\n",
       "fit_time      7.271 (+/- 0.306)  6.866 (+/- 0.146)  17.725 (+/- 0.089)   \n",
       "score_time    0.613 (+/- 0.024)  0.656 (+/- 0.025)   0.653 (+/- 0.018)   \n",
       "test_score    0.964 (+/- 0.007)  0.869 (+/- 0.008)   0.973 (+/- 0.005)   \n",
       "train_score   0.998 (+/- 0.000)  0.889 (+/- 0.003)   1.000 (+/- 0.000)   \n",
       "\n",
       "                      Cat boost  \n",
       "fit_time     27.518 (+/- 0.807)  \n",
       "score_time    0.632 (+/- 0.033)  \n",
       "test_score    0.976 (+/- 0.004)  \n",
       "train_score   0.990 (+/- 0.000)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, value in models.items():\n",
    "    print(f\"Start {name}!\")\n",
    "    results[name] = mean_std_cross_val_scores(\n",
    "        value, X_train, y_train, cv=10, return_train_score=True\n",
    "    )\n",
    "    print(f\"Done {name}!\")\n",
    "    display(pd.DataFrame(results))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f466aafd-1fe9-4789-ab49-3111e452f5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('columntransformer',\n",
       "                 ColumnTransformer(transformers=[('pipeline-1',\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('functiontransformer',\n",
       "                                                                   FunctionTransformer(func=<function reshape at 0x10fd91750>,\n",
       "                                                                                       kw_args={'newshape': -1})),\n",
       "                                                                  ('countvectorizer',\n",
       "                                                                   CountVectorizer(max_features=1000,\n",
       "                                                                                   stop_words='english'))]),\n",
       "                                                  ['title'])...\n",
       "                                                  Pipeline(steps=[('simpleimputer',\n",
       "                                                                   SimpleImputer(strategy='most_frequent')),\n",
       "                                                                  ('functiontransformer',\n",
       "                                                                   FunctionTransformer(func=<function reshape at 0x10fd91750>,\n",
       "                                                                                       kw_args={'newshape': -1})),\n",
       "                                                                  ('countvectorizer',\n",
       "                                                                   CountVectorizer(max_features=1000,\n",
       "                                                                                   stop_words='english'))]),\n",
       "                                                  ['text']),\n",
       "                                                 ('drop', 'drop',\n",
       "                                                  ['id', 'author'])])),\n",
       "                ('logisticregression', LogisticRegression(max_iter=100000))])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "6611971b-2d89-49f3-9f5c-18c894fecbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.32858657,  0.03769198, -0.22062586, ..., -0.28690671,\n",
       "        -0.57266297,  0.72208062]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lr.named_steps[\"logisticregression\"].coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0cf18209-008f-4ac7-bfef-2dfc10c30c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Importance</th>\n",
       "      <th>abs_Importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>breitbart</th>\n",
       "      <td>-8.652039</td>\n",
       "      <td>8.652039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>york</th>\n",
       "      <td>-6.238666</td>\n",
       "      <td>6.238666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>000</th>\n",
       "      <td>-4.560829</td>\n",
       "      <td>4.560829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>times</th>\n",
       "      <td>-3.522039</td>\n",
       "      <td>3.522039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>follow</th>\n",
       "      <td>-2.307340</td>\n",
       "      <td>2.307340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>comment</th>\n",
       "      <td>1.835264</td>\n",
       "      <td>1.835264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>october</th>\n",
       "      <td>1.803078</td>\n",
       "      <td>1.803078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>migrants</th>\n",
       "      <td>-1.698890</td>\n",
       "      <td>1.698890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>islamic</th>\n",
       "      <td>-1.664931</td>\n",
       "      <td>1.664931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>migrant</th>\n",
       "      <td>-1.654532</td>\n",
       "      <td>1.654532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nan</th>\n",
       "      <td>1.604842</td>\n",
       "      <td>1.604842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share</th>\n",
       "      <td>1.582791</td>\n",
       "      <td>1.582791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>-1.495447</td>\n",
       "      <td>1.495447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breitbart</th>\n",
       "      <td>-1.423395</td>\n",
       "      <td>1.423395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>french</th>\n",
       "      <td>-1.403862</td>\n",
       "      <td>1.403862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>street</th>\n",
       "      <td>-1.334662</td>\n",
       "      <td>1.334662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>inauguration</th>\n",
       "      <td>-1.307659</td>\n",
       "      <td>1.307659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>california</th>\n",
       "      <td>1.263420</td>\n",
       "      <td>1.263420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palestinian</th>\n",
       "      <td>-1.239399</td>\n",
       "      <td>1.239399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>november</th>\n",
       "      <td>1.217751</td>\n",
       "      <td>1.217751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>-1.214862</td>\n",
       "      <td>1.214862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pope</th>\n",
       "      <td>-1.203107</td>\n",
       "      <td>1.203107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>israel</th>\n",
       "      <td>-1.202273</td>\n",
       "      <td>1.202273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>march</th>\n",
       "      <td>-1.160926</td>\n",
       "      <td>1.160926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>jewish</th>\n",
       "      <td>-1.137710</td>\n",
       "      <td>1.137710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>february</th>\n",
       "      <td>-1.123466</td>\n",
       "      <td>1.123466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>report</th>\n",
       "      <td>-1.117527</td>\n",
       "      <td>1.117527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>death</th>\n",
       "      <td>-1.109556</td>\n",
       "      <td>1.109556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>twitter</th>\n",
       "      <td>-1.100445</td>\n",
       "      <td>1.100445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>terror</th>\n",
       "      <td>-1.087021</td>\n",
       "      <td>1.087021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Importance  abs_Importance\n",
       "breitbart      -8.652039        8.652039\n",
       "york           -6.238666        6.238666\n",
       "000            -4.560829        4.560829\n",
       "times          -3.522039        3.522039\n",
       "follow         -2.307340        2.307340\n",
       "comment         1.835264        1.835264\n",
       "october         1.803078        1.803078\n",
       "migrants       -1.698890        1.698890\n",
       "islamic        -1.664931        1.664931\n",
       "migrant        -1.654532        1.654532\n",
       "nan             1.604842        1.604842\n",
       "share           1.582791        1.582791\n",
       "2017           -1.495447        1.495447\n",
       "breitbart      -1.423395        1.423395\n",
       "french         -1.403862        1.403862\n",
       "street         -1.334662        1.334662\n",
       "inauguration   -1.307659        1.307659\n",
       "california      1.263420        1.263420\n",
       "palestinian    -1.239399        1.239399\n",
       "november        1.217751        1.217751\n",
       "state          -1.214862        1.214862\n",
       "pope           -1.203107        1.203107\n",
       "israel         -1.202273        1.202273\n",
       "march          -1.160926        1.160926\n",
       "jewish         -1.137710        1.137710\n",
       "february       -1.123466        1.123466\n",
       "report         -1.117527        1.117527\n",
       "death          -1.109556        1.109556\n",
       "twitter        -1.100445        1.100445\n",
       "terror         -1.087021        1.087021"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    \"Importance\": pipe_lr.named_steps[\"logisticregression\"].coef_.flatten(),\n",
    "    \"abs_Importance\": np.abs(pipe_lr.named_steps[\"logisticregression\"].coef_.flatten())\n",
    "}\n",
    "\n",
    "pd.DataFrame(data=data, index=new_cols).sort_values(\n",
    "    by=\"abs_Importance\", ascending=False\n",
    ")[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d56b7d8-1379-47ec-ae20-669650dea5f7",
   "metadata": {},
   "source": [
    "#### HyperParam Tune best performing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61d8513-fb39-4435-972e-3cebda3a10b4",
   "metadata": {},
   "source": [
    "### Prediction and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b931e18-6491-449b-a4f9-ce0bca144aa9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fake_news]",
   "language": "python",
   "name": "conda-env-fake_news-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
